{"id": 9, "category": "devops", "subcategory": "pipeline", "difficulty": "expert", "title": "Advanced CI/CD Pipeline with MLOps", "prompt": "Como DevOps Lead con experiencia en MLOps, diseÃ±a y implementa un pipeline de CI/CD completo que integre desarrollo tradicional con machine learning operations para una plataforma de alquiler vacacional que incluya automated testing, model deployment, A/B testing, y monitoring.\n\nðŸ”„ PIPELINE ARCHITECTURE:\n\n**Multi-Stage Pipeline Design:**\n```yaml\n# .github/workflows/complete-pipeline.yml\nname: Complete MLOps Pipeline\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n  schedule:\n    - cron: '0 2 * * *'  # Daily model retraining\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: vacation-rental/api\n  MODEL_REGISTRY: s3://models.vacation-rental.com\n  KUBECONFIG_FILE: ${{ secrets.KUBE_CONFIG }}\n\njobs:\n  code-quality:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-node@v4\n      with:\n        node-version: '20'\n        cache: 'npm'\n    \n    - name: Install dependencies\n      run: npm ci\n    \n    - name: Lint code\n      run: |\n        npm run lint\n        npm run lint:security\n        npm run type-check\n    \n    - name: Run unit tests\n      run: npm run test:unit -- --coverage\n    \n    - name: SonarCloud analysis\n      uses: SonarSource/sonarcloud-github-action@master\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}\n    \n    - name: Upload coverage to Codecov\n      uses: codecov/codecov-action@v3\n\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    \n    - name: Run Trivy vulnerability scanner\n      uses: aquasecurity/trivy-action@master\n      with:\n        scan-type: 'fs'\n        format: 'sarif'\n        output: 'trivy-results.sarif'\n    \n    - name: Upload Trivy scan results\n      uses: github/codeql-action/upload-sarif@v2\n      with:\n        sarif_file: 'trivy-results.sarif'\n    \n    - name: OWASP Dependency Check\n      uses: dependency-check/Dependency-Check_Action@main\n      with:\n        project: 'vacation-rental'\n        path: '.'\n        format: 'ALL'\n\n  integration-tests:\n    runs-on: ubuntu-latest\n    needs: [code-quality]\n    services:\n      postgres:\n        image: postgres:15\n        env:\n          POSTGRES_PASSWORD: postgres\n          POSTGRES_DB: test_db\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n      redis:\n        image: redis:7\n        options: >-\n          --health-cmd \"redis-cli ping\"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n    \n    steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-node@v4\n      with:\n        node-version: '20'\n        cache: 'npm'\n    \n    - name: Install dependencies\n      run: npm ci\n    \n    - name: Run integration tests\n      run: npm run test:integration\n      env:\n        DATABASE_URL: postgres://postgres:postgres@localhost:5432/test_db\n        REDIS_URL: redis://localhost:6379\n    \n    - name: API contract testing\n      run: |\n        npm run start:test &\n        sleep 30\n        npm run test:contract\n        pkill -f \"npm run start:test\"\n\n  model-validation:\n    runs-on: ubuntu-latest\n    if: contains(github.event.head_commit.message, '[model]') || github.event_name == 'schedule'\n    steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n        cache: 'pip'\n    \n    - name: Install ML dependencies\n      run: |\n        pip install -r ml/requirements.txt\n        pip install pytest pytest-cov mlflow\n    \n    - name: Data validation\n      run: |\n        python ml/scripts/validate_data.py --data-path data/training\n        python ml/scripts/check_data_drift.py\n    \n    - name: Model training\n      run: |\n        python ml/scripts/train_model.py --config ml/configs/production.yaml\n    \n    - name: Model evaluation\n      run: |\n        python ml/scripts/evaluate_model.py --model-path models/latest\n        python ml/scripts/generate_model_report.py\n    \n    - name: Model testing\n      run: |\n        pytest ml/tests/ --cov=ml --cov-report=xml\n    \n    - name: Upload model artifacts\n      if: success()\n      run: |\n        aws s3 sync models/ $MODEL_REGISTRY/$(git rev-parse HEAD)/\n        mlflow models serve -m $MODEL_REGISTRY/$(git rev-parse HEAD) --no-conda\n\n  build-and-push:\n    runs-on: ubuntu-latest\n    needs: [code-quality, security-scan, integration-tests]\n    if: github.ref == 'refs/heads/main'\n    outputs:\n      image-digest: ${{ steps.build.outputs.digest }}\n    steps:\n    - uses: actions/checkout@v4\n    \n    - name: Set up Docker Buildx\n      uses: docker/setup-buildx-action@v3\n    \n    - name: Log in to Container Registry\n      uses: docker/login-action@v3\n      with:\n        registry: ${{ env.REGISTRY }}\n        username: ${{ github.actor }}\n        password: ${{ secrets.GITHUB_TOKEN }}\n    \n    - name: Extract metadata\n      id: meta\n      uses: docker/metadata-action@v5\n      with:\n        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n        tags: |\n          type=ref,event=branch\n          type=ref,event=pr\n          type=sha,prefix={{branch}}-\n          type=raw,value=latest,enable={{is_default_branch}}\n    \n    - name: Build and push Docker image\n      id: build\n      uses: docker/build-push-action@v5\n      with:\n        context: .\n        platforms: linux/amd64,linux/arm64\n        push: true\n        tags: ${{ steps.meta.outputs.tags }}\n        labels: ${{ steps.meta.outputs.labels }}\n        cache-from: type=gha\n        cache-to: type=gha,mode=max\n        build-args: |\n          BUILD_VERSION=${{ github.sha }}\n          BUILD_DATE=${{ github.event.head_commit.timestamp }}\n\n  deploy-staging:\n    runs-on: ubuntu-latest\n    needs: [build-and-push, model-validation]\n    if: github.ref == 'refs/heads/main'\n    environment: staging\n    steps:\n    - uses: actions/checkout@v4\n    \n    - name: Setup kubectl\n      uses: azure/k8s-set-context@v3\n      with:\n        method: kubeconfig\n        kubeconfig: ${{ secrets.KUBE_CONFIG }}\n    \n    - name: Deploy to staging\n      run: |\n        cd k8s/overlays/staging\n        kustomize edit set image ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}\n        kubectl apply -k .\n    \n    - name: Wait for deployment\n      run: |\n        kubectl rollout status deployment/api-server -n vacation-rental-staging --timeout=300s\n    \n    - name: Run smoke tests\n      run: |\n        kubectl port-forward svc/api-server 3000:3000 -n vacation-rental-staging &\n        sleep 10\n        npm run test:smoke -- --base-url http://localhost:3000\n        pkill -f \"kubectl port-forward\"\n\n  performance-tests:\n    runs-on: ubuntu-latest\n    needs: [deploy-staging]\n    steps:\n    - uses: actions/checkout@v4\n    \n    - name: Install k6\n      run: |\n        sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69\n        echo \"deb https://dl.k6.io/deb stable main\" | sudo tee /etc/apt/sources.list.d/k6.list\n        sudo apt-get update\n        sudo apt-get install k6\n    \n    - name: Run performance tests\n      run: |\n        k6 run --out json=performance-results.json performance/load-test.js\n    \n    - name: Performance regression check\n      run: |\n        python scripts/analyze_performance.py --results performance-results.json --baseline performance/baseline.json\n    \n    - name: Upload performance results\n      uses: actions/upload-artifact@v3\n      with:\n        name: performance-results\n        path: performance-results.json\n\n  canary-deployment:\n    runs-on: ubuntu-latest\n    needs: [performance-tests]\n    if: github.ref == 'refs/heads/main'\n    environment: production\n    steps:\n    - uses: actions/checkout@v4\n    \n    - name: Setup kubectl\n      uses: azure/k8s-set-context@v3\n      with:\n        method: kubeconfig\n        kubeconfig: ${{ secrets.KUBE_CONFIG }}\n    \n    - name: Deploy canary (10%)\n      run: |\n        cd k8s/overlays/production\n        kustomize edit set image ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}\n        kubectl apply -k canary/\n        kubectl patch deployment api-server-canary -n vacation-rental-prod -p '{\"spec\":{\"replicas\":1}}'\n    \n    - name: Monitor canary metrics\n      run: |\n        python scripts/monitor_canary.py --duration 300 --error-threshold 0.01 --latency-threshold 500\n    \n    - name: Promote canary if successful\n      run: |\n        if [ \"$?\" -eq 0 ]; then\n          kubectl patch deployment api-server -n vacation-rental-prod -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"api-server\",\"image\":\"${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}\"}]}}}}'\n          kubectl rollout status deployment/api-server -n vacation-rental-prod --timeout=300s\n          kubectl scale deployment api-server-canary -n vacation-rental-prod --replicas=0\n        else\n          echo \"Canary deployment failed, rolling back\"\n          kubectl scale deployment api-server-canary -n vacation-rental-prod --replicas=0\n          exit 1\n        fi\n\n  model-deployment:\n    runs-on: ubuntu-latest\n    needs: [canary-deployment]\n    if: contains(github.event.head_commit.message, '[model]') || github.event_name == 'schedule'\n    steps:\n    - uses: actions/checkout@v4\n    \n    - name: Deploy model to production\n      run: |\n        python ml/scripts/deploy_model.py \\\n          --model-uri $MODEL_REGISTRY/$(git rev-parse HEAD) \\\n          --deployment-name price-prediction-prod \\\n          --traffic-percentage 10\n    \n    - name: Monitor model performance\n      run: |\n        python ml/scripts/monitor_model.py \\\n          --deployment price-prediction-prod \\\n          --duration 1800 \\\n          --accuracy-threshold 0.85\n    \n    - name: A/B test results\n      run: |\n        python ml/scripts/ab_test_analysis.py \\\n          --control-model price-prediction-current \\\n          --treatment-model price-prediction-prod \\\n          --significance-level 0.05\n\n  cleanup:\n    runs-on: ubuntu-latest\n    needs: [model-deployment]\n    if: always()\n    steps:\n    - name: Clean up old images\n      run: |\n        # Keep only last 10 images\n        echo \"Cleaning up old container images...\"\n    \n    - name: Archive test results\n      run: |\n        echo \"Archiving test results and artifacts...\"\n    \n    - name: Send notifications\n      run: |\n        python scripts/send_notifications.py \\\n          --status ${{ job.status }} \\\n          --commit ${{ github.sha }} \\\n          --slack-webhook ${{ secrets.SLACK_WEBHOOK }}\n```\n\n**MLOps Pipeline Components:**\n```python\n# ml/scripts/train_model.py\nimport mlflow\nimport mlflow.sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score\nimport pandas as pd\nimport joblib\nimport argparse\nimport yaml\nfrom datetime import datetime\n\nclass ModelTrainer:\n    def __init__(self, config_path):\n        with open(config_path, 'r') as f:\n            self.config = yaml.safe_load(f)\n        \n        mlflow.set_tracking_uri(self.config['mlflow']['tracking_uri'])\n        mlflow.set_experiment(self.config['mlflow']['experiment_name'])\n    \n    def load_and_preprocess_data(self):\n        \"\"\"Load and preprocess training data\"\"\"\n        data = pd.read_csv(self.config['data']['training_path'])\n        \n        # Feature engineering\n        data = self.engineer_features(data)\n        \n        # Data validation\n        self.validate_data(data)\n        \n        X = data[self.config['features']['input_columns']]\n        y = data[self.config['features']['target_column']]\n        \n        return train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    def engineer_features(self, data):\n        \"\"\"Apply feature engineering transformations\"\"\"\n        # Location-based features\n        data['distance_to_beach'] = self.calculate_distance_to_beach(data)\n        data['distance_to_center'] = self.calculate_distance_to_center(data)\n        \n        # Temporal features\n        data['month'] = pd.to_datetime(data['date']).dt.month\n        data['day_of_week'] = pd.to_datetime(data['date']).dt.dayofweek\n        data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n        \n        # Property features\n        data['amenity_score'] = self.calculate_amenity_score(data)\n        data['review_sentiment'] = self.analyze_review_sentiment(data)\n        \n        return data\n    \n    def train_model(self):\n        \"\"\"Train the pricing model\"\"\"\n        X_train, X_test, y_train, y_test = self.load_and_preprocess_data()\n        \n        with mlflow.start_run():\n            # Log parameters\n            model_params = self.config['model']['parameters']\n            mlflow.log_params(model_params)\n            \n            # Train model\n            model = RandomForestRegressor(**model_params)\n            model.fit(X_train, y_train)\n            \n            # Evaluate model\n            y_pred = model.predict(X_test)\n            mae = mean_absolute_error(y_test, y_pred)\n            r2 = r2_score(y_test, y_pred)\n            \n            # Log metrics\n            mlflow.log_metrics({\n                'mae': mae,\n                'r2': r2,\n                'training_samples': len(X_train),\n                'test_samples': len(X_test)\n            })\n            \n            # Log model\n            mlflow.sklearn.log_model(\n                model, \n                \"model\",\n                registered_model_name=\"price_prediction_model\"\n            )\n            \n            # Save model artifacts\n            model_path = f\"models/price_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n            joblib.dump(model, model_path)\n            mlflow.log_artifact(model_path)\n            \n            print(f\"Model trained successfully with MAE: {mae:.2f}, R2: {r2:.3f}\")\n            return model, mae, r2\n\n# ml/scripts/monitor_model.py\nimport requests\nimport time\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass ModelMonitor:\n    def __init__(self, deployment_name, monitoring_config):\n        self.deployment_name = deployment_name\n        self.config = monitoring_config\n        self.metrics_collector = MetricsCollector()\n        self.alert_manager = AlertManager()\n    \n    def monitor_performance(self, duration_minutes=30):\n        \"\"\"Monitor model performance in real-time\"\"\"\n        start_time = datetime.now()\n        end_time = start_time + timedelta(minutes=duration_minutes)\n        \n        metrics = []\n        \n        while datetime.now() < end_time:\n            current_metrics = self.collect_current_metrics()\n            metrics.append(current_metrics)\n            \n            # Check for anomalies\n            if self.detect_anomalies(current_metrics):\n                self.alert_manager.send_alert(\n                    f\"Anomaly detected in {self.deployment_name}\",\n                    current_metrics\n                )\n            \n            # Check for drift\n            if len(metrics) > 10:\n                if self.detect_data_drift(metrics[-10:]):\n                    self.alert_manager.send_alert(\n                        f\"Data drift detected in {self.deployment_name}\",\n                        metrics[-10:]\n                    )\n            \n            time.sleep(60)  # Check every minute\n        \n        return self.generate_monitoring_report(metrics)\n    \n    def detect_anomalies(self, current_metrics):\n        \"\"\"Detect performance anomalies\"\"\"\n        # Check prediction latency\n        if current_metrics['avg_latency'] > self.config['thresholds']['max_latency']:\n            return True\n        \n        # Check error rate\n        if current_metrics['error_rate'] > self.config['thresholds']['max_error_rate']:\n            return True\n        \n        # Check prediction distribution\n        historical_mean = self.get_historical_prediction_mean()\n        current_mean = current_metrics['prediction_mean']\n        \n        if abs(current_mean - historical_mean) > self.config['thresholds']['prediction_drift']:\n            return True\n        \n        return False\n    \n    def detect_data_drift(self, recent_metrics):\n        \"\"\"Detect data drift using statistical tests\"\"\"\n        historical_data = self.get_historical_input_data()\n        recent_data = self.get_recent_input_data()\n        \n        # Kolmogorov-Smirnov test for each feature\n        drift_detected = False\n        \n        for feature in self.config['monitored_features']:\n            ks_statistic, p_value = stats.ks_2samp(\n                historical_data[feature], \n                recent_data[feature]\n            )\n            \n            if p_value < self.config['drift_detection']['significance_level']:\n                print(f\"Data drift detected in feature {feature}: p-value = {p_value}\")\n                drift_detected = True\n        \n        return drift_detected\n    \n    def generate_monitoring_report(self, metrics):\n        \"\"\"Generate comprehensive monitoring report\"\"\"\n        df = pd.DataFrame(metrics)\n        \n        report = {\n            'summary': {\n                'avg_latency': df['avg_latency'].mean(),\n                'p95_latency': df['avg_latency'].quantile(0.95),\n                'error_rate': df['error_rate'].mean(),\n                'total_predictions': df['prediction_count'].sum(),\n                'uptime_percentage': (1 - df['error_rate'].mean()) * 100\n            },\n            'trends': {\n                'latency_trend': self.calculate_trend(df['avg_latency']),\n                'error_trend': self.calculate_trend(df['error_rate']),\n                'volume_trend': self.calculate_trend(df['prediction_count'])\n            },\n            'recommendations': self.generate_recommendations(df)\n        }\n        \n        # Generate visualizations\n        self.create_monitoring_dashboard(df)\n        \n        return report\n```\n\n**Infrastructure as Code:**\n```terraform\n# infrastructure/main.tf\nterraform {\n  required_version = \">= 1.0\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~> 2.20\"\n    }\n  }\n  \n  backend \"s3\" {\n    bucket = \"vacation-rental-terraform-state\"\n    key    = \"production/terraform.tfstate\"\n    region = \"us-west-2\"\n  }\n}\n\n# EKS Cluster\nmodule \"eks\" {\n  source = \"terraform-aws-modules/eks/aws\"\n  \n  cluster_name    = \"vacation-rental-prod\"\n  cluster_version = \"1.27\"\n  \n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n  \n  cluster_addons = {\n    coredns = {\n      most_recent = true\n    }\n    kube-proxy = {\n      most_recent = true\n    }\n    vpc-cni = {\n      most_recent = true\n    }\n    aws-ebs-csi-driver = {\n      most_recent = true\n    }\n  }\n  \n  eks_managed_node_groups = {\n    system = {\n      instance_types = [\"t3.medium\"]\n      min_size       = 3\n      max_size       = 6\n      desired_size   = 3\n      \n      labels = {\n        role = \"system\"\n      }\n      \n      taints = {\n        system = {\n          key    = \"system\"\n          value  = \"true\"\n          effect = \"NO_SCHEDULE\"\n        }\n      }\n    }\n    \n    application = {\n      instance_types = [\"t3.large\", \"t3.xlarge\"]\n      min_size       = 3\n      max_size       = 20\n      desired_size   = 5\n      \n      labels = {\n        role = \"application\"\n      }\n    }\n    \n    ml_workloads = {\n      instance_types = [\"c5.2xlarge\", \"c5.4xlarge\"]\n      min_size       = 0\n      max_size       = 10\n      desired_size   = 2\n      \n      labels = {\n        role = \"ml\"\n        workload = \"training\"\n      }\n      \n      taints = {\n        ml = {\n          key    = \"ml-workload\"\n          value  = \"true\"\n          effect = \"NO_SCHEDULE\"\n        }\n      }\n    }\n  }\n}\n\n# MLflow Tracking Server\nresource \"aws_ecs_cluster\" \"mlflow\" {\n  name = \"mlflow-tracking\"\n  \n  setting {\n    name  = \"containerInsights\"\n    value = \"enabled\"\n  }\n}\n\nresource \"aws_ecs_service\" \"mlflow_tracking\" {\n  name            = \"mlflow-tracking\"\n  cluster         = aws_ecs_cluster.mlflow.id\n  task_definition = aws_ecs_task_definition.mlflow.arn\n  desired_count   = 2\n  \n  load_balancer {\n    target_group_arn = aws_lb_target_group.mlflow.arn\n    container_name   = \"mlflow\"\n    container_port   = 5000\n  }\n}\n\n# Model Registry S3\nresource \"aws_s3_bucket\" \"model_registry\" {\n  bucket = \"vacation-rental-model-registry\"\n}\n\nresource \"aws_s3_bucket_versioning\" \"model_registry\" {\n  bucket = aws_s3_bucket.model_registry.id\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\n# Monitoring Infrastructure\nmodule \"prometheus\" {\n  source = \"./modules/prometheus\"\n  \n  cluster_name = module.eks.cluster_name\n  vpc_id       = module.vpc.vpc_id\n  subnet_ids   = module.vpc.private_subnets\n}\n\nmodule \"grafana\" {\n  source = \"./modules/grafana\"\n  \n  prometheus_endpoint = module.prometheus.endpoint\n  cluster_name       = module.eks.cluster_name\n}\n```\n\nIncluye automated rollbacks, feature flags, canary deployments, model A/B testing, comprehensive monitoring, cost optimization, security scanning, y documentation completa para todo el pipeline.", "tags": ["cicd", "mlops", "pipeline", "automation", "kubernetes", "terraform"], "complexity": 10, "estimated_time": "120-180 hours", "use_cases": ["enterprise_pipeline", "mlops", "automated_deployment"]}
{"id": 10, "category": "security", "subcategory": "penetration-testing", "difficulty": "expert", "title": "Comprehensive Security Assessment Framework", "prompt": "Como Cybersecurity Expert y Ethical Hacker certificado, desarrolla un framework completo de security assessment para una plataforma de alquiler vacacional que incluya automated vulnerability scanning, penetration testing, security monitoring, incident response, y compliance frameworks.\n\nðŸ”’ SECURITY ASSESSMENT FRAMEWORK:\n\n**Automated Vulnerability Scanning:**\n```python\n# security/vulnerability_scanner.py\nimport nmap\nimport requests\nimport json\nimport sqlite3\nfrom datetime import datetime\nfrom typing import List, Dict, Any\nimport subprocess\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass VulnerabilityScanner:\n    def __init__(self, config_path: str):\n        with open(config_path, 'r') as f:\n            self.config = json.load(f)\n        \n        self.db = sqlite3.connect('security_assessment.db')\n        self.init_database()\n        self.nm = nmap.PortScanner()\n    \n    def init_database(self):\n        \"\"\"Initialize security assessment database\"\"\"\n        cursor = self.db.cursor()\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS vulnerability_scans (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                scan_type TEXT NOT NULL,\n                target TEXT NOT NULL,\n                severity TEXT NOT NULL,\n                vulnerability TEXT NOT NULL,\n                description TEXT,\n                remediation TEXT,\n                cvss_score REAL,\n                cve_id TEXT,\n                scan_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                status TEXT DEFAULT 'open'\n            )\n        ''')\n        \n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS security_metrics (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                metric_name TEXT NOT NULL,\n                metric_value TEXT NOT NULL,\n                metric_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        self.db.commit()\n    \n    def network_scan(self, target_range: str) -> Dict[str, Any]:\n        \"\"\"Perform comprehensive network scanning\"\"\"\n        print(f\"Starting network scan for {target_range}\")\n        \n        # TCP SYN scan for common ports\n        tcp_results = self.nm.scan(\n            target_range, \n            '1-65535', \n            '-sS -sV -O --script vuln'\n        )\n        \n        # UDP scan for critical services\n        udp_results = self.nm.scan(\n            target_range,\n            '53,67,68,69,123,161,162,514',\n            '-sU'\n        )\n        \n        vulnerabilities = []\n        \n        for host in tcp_results['scan']:\n            host_info = tcp_results['scan'][host]\n            \n            # Check for open ports\n            if 'tcp' in host_info:\n                for port, port_info in host_info['tcp'].items():\n                    if port_info['state'] == 'open':\n                        service = port_info.get('name', 'unknown')\n                        version = port_info.get('version', '')\n                        \n                        # Check for known vulnerable services\n                        vuln = self.check_service_vulnerabilities(\n                            service, version, port\n                        )\n                        if vuln:\n                            vulnerabilities.extend(vuln)\n            \n            # Analyze script results for vulnerabilities\n            if 'hostscript' in host_info:\n                for script in host_info['hostscript']:\n                    if 'vuln' in script['id']:\n                        vulnerabilities.append({\n                            'type': 'network',\n                            'host': host,\n                            'vulnerability': script['id'],\n                            'description': script['output'],\n                            'severity': self.assess_severity(script['output']),\n                            'remediation': self.get_remediation(script['id'])\n                        })\n        \n        return {\n            'tcp_scan': tcp_results,\n            'udp_scan': udp_results,\n            'vulnerabilities': vulnerabilities\n        }\n    \n    def web_application_scan(self, target_url: str) -> Dict[str, Any]:\n        \"\"\"Comprehensive web application security testing\"\"\"\n        vulnerabilities = []\n        \n        # OWASP Top 10 Testing\n        vulnerabilities.extend(self.test_sql_injection(target_url))\n        vulnerabilities.extend(self.test_xss(target_url))\n        vulnerabilities.extend(self.test_csrf(target_url))\n        vulnerabilities.extend(self.test_authentication(target_url))\n        vulnerabilities.extend(self.test_authorization(target_url))\n        vulnerabilities.extend(self.test_security_headers(target_url))\n        vulnerabilities.extend(self.test_ssl_tls(target_url))\n        vulnerabilities.extend(self.test_information_disclosure(target_url))\n        \n        # API Security Testing\n        api_endpoints = self.discover_api_endpoints(target_url)\n        for endpoint in api_endpoints:\n            vulnerabilities.extend(self.test_api_security(endpoint))\n        \n        return {\n            'target': target_url,\n            'vulnerabilities': vulnerabilities,\n            'api_endpoints': api_endpoints\n        }\n    \n    def test_sql_injection(self, target_url: str) -> List[Dict[str, Any]]:\n        \"\"\"Test for SQL injection vulnerabilities\"\"\"\n        vulnerabilities = []\n        \n        # Common SQL injection payloads\n        payloads = [\n            \"' OR '1'='1\",\n            \"' UNION SELECT NULL--\",\n            \"'; DROP TABLE users--\",\n            \"' OR 1=1#\",\n            \"admin'--\",\n            \"' OR 'a'='a\",\n            \"') OR ('1'='1\"\n        ]\n        \n        # Test common injection points\n        injection_points = [\n            '/search?q=',\n            '/login?username=',\n            '/api/properties?id=',\n            '/booking?property_id='\n        ]\n        \n        for point in injection_points:\n            for payload in payloads:\n                test_url = f\"{target_url}{point}{payload}\"\n                \n                try:\n                    response = requests.get(test_url, timeout=10)\n                    \n                    # Check for SQL error messages\n                    error_patterns = [\n                        'SQL syntax error',\n                        'mysql_fetch_array',\n                        'ORA-01756',\n                        'Microsoft OLE DB Provider',\n                        'PostgreSQL query failed'\n                    ]\n                    \n                    for pattern in error_patterns:\n                        if pattern.lower() in response.text.lower():\n                            vulnerabilities.append({\n                                'type': 'sql_injection',\n                                'endpoint': point,\n                                'payload': payload,\n                                'severity': 'high',\n                                'description': f'SQL injection vulnerability detected at {point}',\n                                'evidence': pattern,\n                                'remediation': 'Use parameterized queries and input validation'\n                            })\n                            break\n                \n                except requests.RequestException:\n                    continue\n        \n        return vulnerabilities\n    \n    def test_xss(self, target_url: str) -> List[Dict[str, Any]]:\n        \"\"\"Test for Cross-Site Scripting vulnerabilities\"\"\"\n        vulnerabilities = []\n        \n        # XSS payloads\n        payloads = [\n            \"<script>alert('XSS')</script>\",\n            \"javascript:alert('XSS')\",\n            \"<img src=x onerror=alert('XSS')>\",\n            \"<svg onload=alert('XSS')>\",\n            \"'><script>alert('XSS')</script>\",\n            \"\\\"onmouseover=alert('XSS')\\\"\"\n        ]\n        \n        # Test input fields and parameters\n        test_endpoints = [\n            '/search?q=',\n            '/contact?message=',\n            '/review?comment=',\n            '/profile?bio='\n        ]\n        \n        for endpoint in test_endpoints:\n            for payload in payloads:\n                test_url = f\"{target_url}{endpoint}{payload}\"\n                \n                try:\n                    response = requests.get(test_url, timeout=10)\n                    \n                    # Check if payload is reflected in response\n                    if payload in response.text:\n                        vulnerabilities.append({\n                            'type': 'xss',\n                            'endpoint': endpoint,\n                            'payload': payload,\n                            'severity': 'medium',\n                            'description': f'Reflected XSS vulnerability at {endpoint}',\n                            'remediation': 'Implement proper input validation and output encoding'\n                        })\n                \n                except requests.RequestException:\n                    continue\n        \n        return vulnerabilities\n    \n    def test_authentication(self, target_url: str) -> List[Dict[str, Any]]:\n        \"\"\"Test authentication mechanisms\"\"\"\n        vulnerabilities = []\n        \n        # Test for weak password policies\n        weak_passwords = ['password', '123456', 'admin', 'test', 'guest']\n        \n        for password in weak_passwords:\n            login_data = {\n                'username': 'admin',\n                'password': password\n            }\n            \n            try:\n                response = requests.post(\n                    f\"{target_url}/login\",\n                    data=login_data,\n                    timeout=10\n                )\n                \n                if response.status_code == 200 and 'dashboard' in response.text.lower():\n                    vulnerabilities.append({\n                        'type': 'weak_authentication',\n                        'password': password,\n                        'severity': 'high',\n                        'description': f'Weak password policy allows {password}',\n                        'remediation': 'Implement strong password requirements'\n                    })\n            \n            except requests.RequestException:\n                continue\n        \n        # Test for session management issues\n        session_tests = self.test_session_management(target_url)\n        vulnerabilities.extend(session_tests)\n        \n        return vulnerabilities\n    \n    def test_api_security(self, api_endpoint: str) -> List[Dict[str, Any]]:\n        \"\"\"Test API-specific security issues\"\"\"\n        vulnerabilities = []\n        \n        # Test for IDOR (Insecure Direct Object References)\n        idor_tests = self.test_idor(api_endpoint)\n        vulnerabilities.extend(idor_tests)\n        \n        # Test for rate limiting\n        rate_limit_test = self.test_rate_limiting(api_endpoint)\n        if rate_limit_test:\n            vulnerabilities.append(rate_limit_test)\n        \n        # Test for proper authentication\n        auth_test = self.test_api_authentication(api_endpoint)\n        if auth_test:\n            vulnerabilities.append(auth_test)\n        \n        return vulnerabilities\n    \n    def generate_security_report(self, scan_results: Dict[str, Any]) -> str:\n        \"\"\"Generate comprehensive security assessment report\"\"\"\n        report = {\n            'executive_summary': self.generate_executive_summary(scan_results),\n            'vulnerability_details': scan_results.get('vulnerabilities', []),\n            'risk_assessment': self.calculate_risk_score(scan_results),\n            'recommendations': self.generate_recommendations(scan_results),\n            'compliance_status': self.check_compliance(scan_results),\n            'remediation_timeline': self.create_remediation_plan(scan_results)\n        }\n        \n        # Generate HTML report\n        html_report = self.create_html_report(report)\n        \n        # Save to database\n        self.save_scan_results(scan_results)\n        \n        return html_report\n\n# Penetration Testing Framework\nclass PenetrationTester:\n    def __init__(self):\n        self.tools = {\n            'nmap': '/usr/bin/nmap',\n            'sqlmap': '/usr/bin/sqlmap',\n            'gobuster': '/usr/bin/gobuster',\n            'nikto': '/usr/bin/nikto',\n            'metasploit': '/usr/bin/msfconsole'\n        }\n    \n    def automated_penetration_test(self, target: str) -> Dict[str, Any]:\n        \"\"\"Perform automated penetration testing\"\"\"\n        results = {\n            'reconnaissance': self.reconnaissance_phase(target),\n            'scanning': self.scanning_phase(target),\n            'enumeration': self.enumeration_phase(target),\n            'exploitation': self.exploitation_phase(target),\n            'post_exploitation': self.post_exploitation_phase(target)\n        }\n        \n        return results\n    \n    def reconnaissance_phase(self, target: str) -> Dict[str, Any]:\n        \"\"\"Information gathering phase\"\"\"\n        return {\n            'domain_info': self.gather_domain_info(target),\n            'subdomain_enum': self.enumerate_subdomains(target),\n            'email_harvesting': self.harvest_emails(target),\n            'social_media': self.gather_social_media_info(target)\n        }\n    \n    def exploitation_phase(self, target: str) -> Dict[str, Any]:\n        \"\"\"Attempt to exploit identified vulnerabilities\"\"\"\n        exploits = []\n        \n        # SQL Injection exploitation\n        sqli_exploits = self.exploit_sql_injection(target)\n        exploits.extend(sqli_exploits)\n        \n        # File upload vulnerabilities\n        upload_exploits = self.exploit_file_upload(target)\n        exploits.extend(upload_exploits)\n        \n        # Authentication bypass\n        auth_exploits = self.exploit_authentication_bypass(target)\n        exploits.extend(auth_exploits)\n        \n        return {\n            'successful_exploits': exploits,\n            'access_level': self.assess_access_level(exploits),\n            'impact_assessment': self.assess_impact(exploits)\n        }\n```", "tags": ["security", "penetration-testing", "vulnerability", "compliance", "monitoring"], "complexity": 10, "estimated_time": "100-150 hours", "use_cases": ["security_assessment", "compliance", "penetration_testing"]}
{"id": 11, "category": "business", "subcategory": "analytics", "difficulty": "expert", "title": "Advanced Business Intelligence Platform", "prompt": "Como Senior Data Analyst y Business Intelligence Architect, desarrolla una plataforma completa de BI para una empresa de alquiler vacacional que incluya data warehousing, ETL pipelines, real-time analytics, predictive modeling, automated reporting, y executive dashboards.\n\nðŸ“Š BUSINESS INTELLIGENCE ARCHITECTURE:\n\n**Data Warehouse Design:**\n```sql\n-- Data Warehouse Schema (Star Schema)\n-- Fact Tables\nCREATE TABLE fact_bookings (\n    booking_id BIGINT PRIMARY KEY,\n    property_id INT NOT NULL,\n    guest_id INT NOT NULL,\n    host_id INT NOT NULL,\n    check_in_date DATE NOT NULL,\n    check_out_date DATE NOT NULL,\n    booking_date TIMESTAMP NOT NULL,\n    nights INT NOT NULL,\n    guests INT NOT NULL,\n    base_price DECIMAL(10,2) NOT NULL,\n    cleaning_fee DECIMAL(10,2) DEFAULT 0,\n    service_fee DECIMAL(10,2) DEFAULT 0,\n    taxes DECIMAL(10,2) DEFAULT 0,\n    total_amount DECIMAL(10,2) NOT NULL,\n    cancellation_date DATE NULL,\n    cancellation_reason VARCHAR(100) NULL,\n    review_score DECIMAL(3,2) NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    \n    -- Foreign Keys to Dimension Tables\n    FOREIGN KEY (property_id) REFERENCES dim_properties(property_id),\n    FOREIGN KEY (guest_id) REFERENCES dim_guests(guest_id),\n    FOREIGN KEY (host_id) REFERENCES dim_hosts(host_id)\n);\n\nCREATE TABLE fact_property_performance (\n    performance_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    property_id INT NOT NULL,\n    date DATE NOT NULL,\n    occupancy_rate DECIMAL(5,2) NOT NULL,\n    average_daily_rate DECIMAL(10,2) NOT NULL,\n    revenue_per_available_night DECIMAL(10,2) NOT NULL,\n    total_revenue DECIMAL(10,2) NOT NULL,\n    total_bookings INT NOT NULL,\n    average_length_of_stay DECIMAL(4,2) NOT NULL,\n    guest_satisfaction DECIMAL(3,2) NULL,\n    \n    UNIQUE KEY unique_property_date (property_id, date),\n    FOREIGN KEY (property_id) REFERENCES dim_properties(property_id)\n);\n\n-- Dimension Tables\nCREATE TABLE dim_properties (\n    property_id INT PRIMARY KEY,\n    property_name VARCHAR(255) NOT NULL,\n    property_type ENUM('apartment', 'house', 'villa', 'studio', 'loft') NOT NULL,\n    bedrooms INT NOT NULL,\n    bathrooms DECIMAL(3,1) NOT NULL,\n    max_guests INT NOT NULL,\n    city VARCHAR(100) NOT NULL,\n    state VARCHAR(100) NOT NULL,\n    country VARCHAR(100) NOT NULL,\n    latitude DECIMAL(10, 8) NULL,\n    longitude DECIMAL(11, 8) NULL,\n    amenities JSON NULL,\n    listing_date DATE NOT NULL,\n    host_id INT NOT NULL,\n    is_active BOOLEAN DEFAULT TRUE,\n    \n    INDEX idx_location (city, state, country),\n    INDEX idx_property_type (property_type),\n    FOREIGN KEY (host_id) REFERENCES dim_hosts(host_id)\n);\n\nCREATE TABLE dim_guests (\n    guest_id INT PRIMARY KEY,\n    guest_email VARCHAR(255) UNIQUE NOT NULL,\n    first_name VARCHAR(100) NOT NULL,\n    last_name VARCHAR(100) NOT NULL,\n    registration_date DATE NOT NULL,\n    country VARCHAR(100) NOT NULL,\n    age_group ENUM('18-25', '26-35', '36-45', '46-55', '56-65', '65+') NULL,\n    guest_segment ENUM('budget', 'mid-range', 'luxury', 'business') NOT NULL,\n    total_bookings INT DEFAULT 0,\n    lifetime_value DECIMAL(10,2) DEFAULT 0,\n    last_booking_date DATE NULL,\n    \n    INDEX idx_country (country),\n    INDEX idx_segment (guest_segment)\n);\n\nCREATE TABLE dim_hosts (\n    host_id INT PRIMARY KEY,\n    host_email VARCHAR(255) UNIQUE NOT NULL,\n    first_name VARCHAR(100) NOT NULL,\n    last_name VARCHAR(100) NOT NULL,\n    registration_date DATE NOT NULL,\n    country VARCHAR(100) NOT NULL,\n    host_type ENUM('individual', 'business', 'property_manager') NOT NULL,\n    total_properties INT DEFAULT 0,\n    average_rating DECIMAL(3,2) NULL,\n    response_rate DECIMAL(5,2) NULL,\n    is_superhost BOOLEAN DEFAULT FALSE,\n    \n    INDEX idx_host_type (host_type),\n    INDEX idx_country (country)\n);\n\n-- Time Dimension for Time Intelligence\nCREATE TABLE dim_date (\n    date_key DATE PRIMARY KEY,\n    year INT NOT NULL,\n    quarter INT NOT NULL,\n    month INT NOT NULL,\n    month_name VARCHAR(20) NOT NULL,\n    day INT NOT NULL,\n    day_of_week INT NOT NULL,\n    day_name VARCHAR(20) NOT NULL,\n    week_of_year INT NOT NULL,\n    is_weekend BOOLEAN NOT NULL,\n    is_holiday BOOLEAN DEFAULT FALSE,\n    season ENUM('Spring', 'Summer', 'Fall', 'Winter') NOT NULL,\n    \n    INDEX idx_year_month (year, month),\n    INDEX idx_quarter (quarter),\n    INDEX idx_season (season)\n);\n```\n\n**ETL Pipeline Implementation:**\n```python\n# etl/booking_etl.py\nimport pandas as pd\nimport numpy as np\nfrom sqlalchemy import create_engine, text\nfrom datetime import datetime, timedelta\nimport logging\nfrom typing import Dict, List, Any\nimport requests\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.sql_operator import SQLCheckOperator\n\nclass BookingETL:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.source_engine = create_engine(config['source_db_url'])\n        self.target_engine = create_engine(config['target_db_url'])\n        self.logger = logging.getLogger(__name__)\n    \n    def extract_booking_data(self, start_date: str, end_date: str) -> pd.DataFrame:\n        \"\"\"Extract booking data from source systems\"\"\"\n        query = \"\"\"\n        SELECT \n            b.id as booking_id,\n            b.property_id,\n            b.guest_id,\n            p.host_id,\n            b.check_in_date,\n            b.check_out_date,\n            b.created_at as booking_date,\n            DATEDIFF(b.check_out_date, b.check_in_date) as nights,\n            b.guests,\n            b.base_price,\n            b.cleaning_fee,\n            b.service_fee,\n            b.taxes,\n            b.total_amount,\n            b.cancelled_at as cancellation_date,\n            b.cancellation_reason,\n            r.rating as review_score,\n            b.created_at,\n            b.updated_at\n        FROM bookings b\n        JOIN properties p ON b.property_id = p.id\n        LEFT JOIN reviews r ON b.id = r.booking_id\n        WHERE b.created_at BETWEEN %s AND %s\n        \"\"\"\n        \n        df = pd.read_sql_query(\n            query, \n            self.source_engine, \n            params=[start_date, end_date]\n        )\n        \n        self.logger.info(f\"Extracted {len(df)} booking records\")\n        return df\n    \n    def transform_booking_data(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Transform and clean booking data\"\"\"\n        # Data cleaning\n        df = df.dropna(subset=['booking_id', 'property_id', 'guest_id'])\n        \n        # Data type conversions\n        df['check_in_date'] = pd.to_datetime(df['check_in_date'])\n        df['check_out_date'] = pd.to_datetime(df['check_out_date'])\n        df['booking_date'] = pd.to_datetime(df['booking_date'])\n        \n        # Business logic transformations\n        df['nights'] = (df['check_out_date'] - df['check_in_date']).dt.days\n        df['lead_time'] = (df['check_in_date'] - df['booking_date']).dt.days\n        \n        # Handle missing values\n        df['cleaning_fee'] = df['cleaning_fee'].fillna(0)\n        df['service_fee'] = df['service_fee'].fillna(0)\n        df['taxes'] = df['taxes'].fillna(0)\n        \n        # Data validation\n        invalid_nights = df['nights'] <= 0\n        if invalid_nights.sum() > 0:\n            self.logger.warning(f\"Found {invalid_nights.sum()} bookings with invalid nights\")\n            df = df[~invalid_nights]\n        \n        # Calculate derived metrics\n        df['average_daily_rate'] = df['base_price'] / df['nights']\n        df['revenue_per_guest'] = df['total_amount'] / df['guests']\n        \n        return df\n    \n    def load_booking_data(self, df: pd.DataFrame) -> None:\n        \"\"\"Load transformed data into data warehouse\"\"\"\n        try:\n            df.to_sql(\n                'fact_bookings',\n                self.target_engine,\n                if_exists='append',\n                index=False,\n                method='multi'\n            )\n            \n            self.logger.info(f\"Successfully loaded {len(df)} booking records\")\n            \n            # Update data quality metrics\n            self.update_data_quality_metrics('fact_bookings', len(df))\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to load booking data: {str(e)}\")\n            raise\n    \n    def calculate_property_performance(self, date: str) -> pd.DataFrame:\n        \"\"\"Calculate daily property performance metrics\"\"\"\n        query = \"\"\"\n        WITH property_stats AS (\n            SELECT \n                p.property_id,\n                COUNT(CASE WHEN b.check_in_date <= %s AND b.check_out_date > %s THEN 1 END) as occupied_nights,\n                COUNT(b.booking_id) as total_bookings,\n                AVG(b.total_amount / b.nights) as avg_daily_rate,\n                SUM(b.total_amount) as total_revenue,\n                AVG(b.nights) as avg_length_of_stay,\n                AVG(b.review_score) as avg_guest_satisfaction\n            FROM dim_properties p\n            LEFT JOIN fact_bookings b ON p.property_id = b.property_id\n                AND b.check_in_date <= %s AND b.check_out_date > %s\n            WHERE p.is_active = TRUE\n            GROUP BY p.property_id\n        )\n        SELECT \n            property_id,\n            %s as date,\n            (occupied_nights / 1.0) * 100 as occupancy_rate,\n            COALESCE(avg_daily_rate, 0) as average_daily_rate,\n            COALESCE(avg_daily_rate * (occupied_nights / 1.0), 0) as revenue_per_available_night,\n            COALESCE(total_revenue, 0) as total_revenue,\n            COALESCE(total_bookings, 0) as total_bookings,\n            COALESCE(avg_length_of_stay, 0) as average_length_of_stay,\n            COALESCE(avg_guest_satisfaction, 0) as guest_satisfaction\n        FROM property_stats\n        \"\"\"\n        \n        df = pd.read_sql_query(\n            query,\n            self.source_engine,\n            params=[date, date, date, date, date]\n        )\n        \n        return df\n    \n    def run_full_etl(self, start_date: str, end_date: str) -> Dict[str, Any]:\n        \"\"\"Run complete ETL process\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            # Extract\n            raw_data = self.extract_booking_data(start_date, end_date)\n            \n            # Transform\n            clean_data = self.transform_booking_data(raw_data)\n            \n            # Load\n            self.load_booking_data(clean_data)\n            \n            # Calculate performance metrics\n            performance_data = self.calculate_property_performance(end_date)\n            performance_data.to_sql(\n                'fact_property_performance',\n                self.target_engine,\n                if_exists='replace',\n                index=False\n            )\n            \n            end_time = datetime.now()\n            duration = (end_time - start_time).total_seconds()\n            \n            return {\n                'status': 'success',\n                'records_processed': len(clean_data),\n                'duration_seconds': duration,\n                'start_time': start_time.isoformat(),\n                'end_time': end_time.isoformat()\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"ETL process failed: {str(e)}\")\n            return {\n                'status': 'failed',\n                'error': str(e),\n                'duration_seconds': (datetime.now() - start_time).total_seconds()\n            }\n\n# Airflow DAG Definition\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 2,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'vacation_rental_etl',\n    default_args=default_args,\n    description='Vacation Rental Data Warehouse ETL',\n    schedule_interval='0 2 * * *',  # Daily at 2 AM\n    catchup=False,\n    max_active_runs=1\n)\n\ndef run_booking_etl(**context):\n    etl = BookingETL(config)\n    execution_date = context['execution_date'].strftime('%Y-%m-%d')\n    start_date = (context['execution_date'] - timedelta(days=1)).strftime('%Y-%m-%d')\n    \n    result = etl.run_full_etl(start_date, execution_date)\n    return result\n\n# Define tasks\nbooking_etl_task = PythonOperator(\n    task_id='run_booking_etl',\n    python_callable=run_booking_etl,\n    dag=dag\n)\n\ndata_quality_check = SQLCheckOperator(\n    task_id='data_quality_check',\n    sql=\"\"\"\n    SELECT COUNT(*) as record_count\n    FROM fact_bookings \n    WHERE DATE(created_at) = '{{ ds }}'\n    HAVING record_count > 0\n    \"\"\",\n    conn_id='data_warehouse',\n    dag=dag\n)\n\n# Set task dependencies\nbooking_etl_task >> data_quality_check\n```\n\n**Real-time Analytics Engine:**\n```python\n# analytics/real_time_analytics.py\nimport pandas as pd\nimport numpy as np\nfrom kafka import KafkaConsumer, KafkaProducer\nimport json\nimport redis\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any, List\nimport asyncio\nimport websockets\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass RealTimeAnalytics:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.redis_client = redis.Redis(\n            host=config['redis_host'],\n            port=config['redis_port'],\n            db=config['redis_db']\n        )\n        \n        self.kafka_consumer = KafkaConsumer(\n            'booking-events',\n            'property-views',\n            'search-events',\n            bootstrap_servers=config['kafka_servers'],\n            value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n        )\n        \n        self.kafka_producer = KafkaProducer(\n            bootstrap_servers=config['kafka_servers'],\n            value_serializer=lambda x: json.dumps(x).encode('utf-8')\n        )\n        \n        self.metrics_buffer = []\n        self.websocket_clients = set()\n    \n    async def process_real_time_events(self):\n        \"\"\"Process real-time events from Kafka\"\"\"\n        for message in self.kafka_consumer:\n            event_data = message.value\n            topic = message.topic\n            \n            if topic == 'booking-events':\n                await self.process_booking_event(event_data)\n            elif topic == 'property-views':\n                await self.process_view_event(event_data)\n            elif topic == 'search-events':\n                await self.process_search_event(event_data)\n            \n            # Update real-time metrics\n            await self.update_real_time_metrics(topic, event_data)\n            \n            # Broadcast to connected clients\n            await self.broadcast_metrics()\n    \n    async def process_booking_event(self, event: Dict[str, Any]):\n        \"\"\"Process booking-related events\"\"\"\n        event_type = event.get('event_type')\n        \n        if event_type == 'booking_created':\n            await self.handle_new_booking(event)\n        elif event_type == 'booking_cancelled':\n            await self.handle_booking_cancellation(event)\n        elif event_type == 'payment_completed':\n            await self.handle_payment_completion(event)\n    \n    async def handle_new_booking(self, event: Dict[str, Any]):\n        \"\"\"Handle new booking creation\"\"\"\n        booking_data = event['data']\n        \n        # Update real-time counters\n        current_date = datetime.now().strftime('%Y-%m-%d')\n        \n        # Daily booking count\n        self.redis_client.incr(f'bookings:daily:{current_date}')\n        \n        # Hourly booking count\n        current_hour = datetime.now().strftime('%Y-%m-%d:%H')\n        self.redis_client.incr(f'bookings:hourly:{current_hour}')\n        \n        # Revenue tracking\n        revenue_key = f'revenue:daily:{current_date}'\n        self.redis_client.incrbyfloat(revenue_key, float(booking_data['total_amount']))\n        \n        # Property-specific metrics\n        property_id = booking_data['property_id']\n        self.redis_client.incr(f'property:bookings:{property_id}')\n        \n        # Geographic metrics\n        city = booking_data.get('city', 'unknown')\n        self.redis_client.incr(f'bookings:city:{city}')\n        \n        # Send real-time alert for high-value bookings\n        if float(booking_data['total_amount']) > 1000:\n            await self.send_high_value_booking_alert(booking_data)\n    \n    async def calculate_real_time_kpis(self) -> Dict[str, Any]:\n        \"\"\"Calculate real-time KPIs\"\"\"\n        current_date = datetime.now().strftime('%Y-%m-%d')\n        current_hour = datetime.now().strftime('%Y-%m-%d:%H')\n        \n        # Get current metrics from Redis\n        daily_bookings = int(self.redis_client.get(f'bookings:daily:{current_date}') or 0)\n        hourly_bookings = int(self.redis_client.get(f'bookings:hourly:{current_hour}') or 0)\n        daily_revenue = float(self.redis_client.get(f'revenue:daily:{current_date}') or 0)\n        \n        # Calculate trends\n        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n        yesterday_bookings = int(self.redis_client.get(f'bookings:daily:{yesterday}') or 0)\n        yesterday_revenue = float(self.redis_client.get(f'revenue:daily:{yesterday}') or 0)\n        \n        booking_trend = ((daily_bookings - yesterday_bookings) / max(yesterday_bookings, 1)) * 100\n        revenue_trend = ((daily_revenue - yesterday_revenue) / max(yesterday_revenue, 1)) * 100\n        \n        # Calculate conversion rate\n        total_searches = await self.get_daily_searches(current_date)\n        conversion_rate = (daily_bookings / max(total_searches, 1)) * 100\n        \n        # Get top performing properties\n        top_properties = await self.get_top_performing_properties()\n        \n        return {\n            'timestamp': datetime.now().isoformat(),\n            'daily_metrics': {\n                'bookings': daily_bookings,\n                'revenue': round(daily_revenue, 2),\n                'average_booking_value': round(daily_revenue / max(daily_bookings, 1), 2),\n                'conversion_rate': round(conversion_rate, 2)\n            },\n            'trends': {\n                'booking_trend': round(booking_trend, 2),\n                'revenue_trend': round(revenue_trend, 2)\n            },\n            'hourly_metrics': {\n                'bookings': hourly_bookings\n            },\n            'top_properties': top_properties,\n            'geographic_distribution': await self.get_geographic_distribution()\n        }\n    \n    async def generate_predictive_insights(self) -> Dict[str, Any]:\n        \"\"\"Generate predictive insights using real-time data\"\"\"\n        # Get historical patterns\n        hourly_patterns = await self.get_hourly_booking_patterns()\n        daily_patterns = await self.get_daily_booking_patterns()\n        \n        # Predict next hour bookings\n        current_hour = datetime.now().hour\n        predicted_next_hour = self.predict_next_hour_bookings(hourly_patterns, current_hour)\n        \n        # Predict daily total\n        predicted_daily_total = self.predict_daily_total(daily_patterns, current_hour)\n        \n        # Identify trending properties\n        trending_properties = await self.identify_trending_properties()\n        \n        # Market insights\n        market_insights = await self.generate_market_insights()\n        \n        return {\n            'predictions': {\n                'next_hour_bookings': predicted_next_hour,\n                'daily_total_forecast': predicted_daily_total,\n                'confidence_score': 0.85\n            },\n            'trends': {\n                'trending_properties': trending_properties,\n                'emerging_destinations': await self.identify_emerging_destinations()\n            },\n            'insights': market_insights,\n            'recommendations': await self.generate_actionable_recommendations()\n        }\n    \n    async def create_automated_alerts(self, metrics: Dict[str, Any]):\n        \"\"\"Create automated alerts based on real-time metrics\"\"\"\n        alerts = []\n        \n        # Revenue drop alert\n        if metrics['trends']['revenue_trend'] < -20:\n            alerts.append({\n                'type': 'revenue_drop',\n                'severity': 'high',\n                'message': f\"Revenue dropped by {abs(metrics['trends']['revenue_trend']):.1f}% compared to yesterday\",\n                'recommended_action': 'Review pricing strategy and marketing campaigns'\n            })\n        \n        # Low conversion rate alert\n        if metrics['daily_metrics']['conversion_rate'] < 2.0:\n            alerts.append({\n                'type': 'low_conversion',\n                'severity': 'medium',\n                'message': f\"Conversion rate is low at {metrics['daily_metrics']['conversion_rate']:.1f}%\",\n                'recommended_action': 'Optimize search results and property listings'\n            })\n        \n        # High booking velocity alert\n        if metrics['hourly_metrics']['bookings'] > 50:\n            alerts.append({\n                'type': 'high_velocity',\n                'severity': 'info',\n                'message': f\"High booking velocity: {metrics['hourly_metrics']['bookings']} bookings this hour\",\n                'recommended_action': 'Monitor system performance and inventory'\n            })\n        \n        # Send alerts if any\n        if alerts:\n            await self.send_alerts(alerts)\n        \n        return alerts\n```", "tags": ["business-intelligence", "analytics", "data-warehouse", "etl", "dashboards"], "complexity": 10, "estimated_time": "150-200 hours", "use_cases": ["business_intelligence", "analytics_platform", "reporting"]}